{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\application\\Anaconda\\envs\\llama\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.qwen2 import Qwen2ForCausalLM, Qwen2Tokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'E:/Model/Qwen2-0.5B-Instruct'\n",
    "model = Qwen2ForCausalLM.from_pretrained(model_name_or_path, device_map='cuda:0')\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo1\n",
    "1. 调用generate方法，让他一直生成新的token\n",
    "2. generate 方法，本质上是一个循环调用 forward 方法，直到终止（EOS、MAX_TOKENS）\n",
    "\n",
    "3. GQA 的本质是 正常 q 是 n_heads 对应 k, v 的 n_heads ，对应进行矩阵计算， 现在变成 q是 n_heads, k, v变成了 n_k_v_heads < n_heads, 后续后repeat_kv 从 [bs, n_k_v_heads, seq_len, hidden_size] -> [bs, n_k_v_heads* group, seq_len, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[109432, 104130,   9370,  99584, 103852,  45995]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "input_ids tensor([[109432, 104130,   9370,  99584, 103852,  45995]], device='cuda:0')\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "text = \"介绍一下杭州的良睦路\"\n",
    "\n",
    "model_inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "print(model_inputs)\n",
    "\n",
    "for k, v in model_inputs.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[109432, 104130,   9370,  99584, 103852,  45995,  33108, 110192,   3837,\n",
       "         104017, 101127,  99661,  99245,  11319,    220, 100622]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=10)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['和西湖，它们分别代表什么？ 作为']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo2\n",
    "1. 直接基于第一步，生成新token\n",
    "2. forward 方法，每次生成一个token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[109432, 104130,   9370,  99584, 103852,  45995]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs1 = {\n",
    "    'input_ids': torch.tensor([[109432, 104130,   9370,  99584, 103852,  45995]], dtype=torch.long).to(model.device),\n",
    "    'attention_mask':torch.tensor([[1,1,1,1,1,1]], dtype=torch.long).to(model.device)\n",
    "}\n",
    "\n",
    "model_inputs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs1 = model.forward(**model_inputs1, use_cache=True)\n",
    "model_outputs1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 151936])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs1.logits.shape     # [bs, seq_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 151936])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后一个预测的token ，就是下一个token，在词表中的概率\n",
    "model_outputs1.logits[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3837], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最大的一个概率的位置，就是对应词的id \n",
    "model_outputs1.logits[:, -1, :].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'，'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3837])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 24, 2, torch.Size([1, 2, 6, 64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    type(model_outputs1.past_key_values),\n",
    "    len(model_outputs1.past_key_values),    # 24, 是模型层数\n",
    "    len(model_outputs1.past_key_values[0]), # tuple 对象[k_cache, v_cache]，包含第0层的k, v\n",
    "    model_outputs1.past_key_values[0][0].shape,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 6, 64]), torch.Size([1, 2, 6, 64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs1.past_key_values[0][0].shape, model_outputs1.past_key_values[0][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs1.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo3\n",
    "1. 把上一次生成的past kv 拿过来，加上新拼接的token，生成\n",
    "2. 对比demo3 和 demo4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs2 = model.forward(\n",
    "    **{\n",
    "        # 输入[bs, seq_len]，需要是二维度\n",
    "        'input_ids': torch.tensor([[3837]], dtype=torch.long).to(model.device),\n",
    "        'attention_mask':torch.tensor([[1]], dtype=torch.long).to(model.device)\n",
    "    },\n",
    "    past_key_values = model_outputs1.past_key_values\n",
    ")\n",
    "\n",
    "model_outputs2.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo4\n",
    "1. 直接模拟简单粗暴类型的生成方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[109432, 104130,   9370,  99584, 103852,  45995,   3837]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs3 = {\n",
    "    'input_ids':torch.tensor(\n",
    "        [[109432, 104130,   9370,  99584, 103852,  45995, 3837]], dtype=torch.long\n",
    "    ).to(model.device),\n",
    "    'attention_mask':torch.tensor([[1, 1, 1, 1, 1, 1, 1]], dtype=torch.long).to(model.device)\n",
    "}\n",
    "\n",
    "model_inputs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs3 = model(**model_inputs3)\n",
    "model_outputs3.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证demo4和demo3输出的logits是不是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(model_outputs3.logits[:, -1, :], model_outputs2.logits[:, -1, :], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([100630], device='cuda:0'), '包括')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs3.logits[:, -1, :].argmax(dim=-1), tokenizer.decode(model_outputs3.logits[:, -1, :].argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([100630], device='cuda:0'), '包括')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs2.logits[:, -1, :].argmax(dim=-1), tokenizer.decode(model_outputs2.logits[:, -1, :].argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat_kv 函数解读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Copied from transformers.models.llama.modeling_llama.repeat_kv\n",
    "def repeat_kv(hidden_states, n_rep):\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxd\\AppData\\Local\\Temp\\ipykernel_21964\\1015326389.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.tensor(torch.arange(6).reshape([1,2,3,1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0],\n",
       "          [1],\n",
       "          [2]],\n",
       "\n",
       "         [[3],\n",
       "          [4],\n",
       "          [5]]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [bs, num_k_v_heads, seq_len, hid_dim]\n",
    "a = torch.tensor(torch.arange(6).reshape([1,2,3,1]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 3, 1]),\n",
       " tensor([[[[0],\n",
       "           [1],\n",
       "           [2]],\n",
       " \n",
       "          [[0],\n",
       "           [1],\n",
       "           [2]],\n",
       " \n",
       "          [[3],\n",
       "           [4],\n",
       "           [5]],\n",
       " \n",
       "          [[3],\n",
       "           [4],\n",
       "           [5]]]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = repeat_kv(a, 2)  # num_k_v_group=2, a.shape = [bs, num_k_v_heads, seq_len, hid_dim] \n",
    "b.shape, b   # [bs, num_k_v_heads * n_k_v_group, seq_len, hid_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
