{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizer 的训练需要保存如下四个文件\n",
    "- tokenizer.json\n",
    "- vocab.json\n",
    "- merges.txt\n",
    "- tokenizer_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders, models, pre_tokenizers, trainers, Tokenizer\n",
    "import os \n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，一般返回一个迭代器，后续训练 tokenizer 需要传入一个迭代器对象\n",
    "def read_data(path):\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            res = data['text'].replace('<|endoftext|>','')\n",
    "            yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE 分词器\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义特殊tokens\n",
    "special_tokens = ['<pad>', '<unk>', '<s>', '</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化训练器 \n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=6400,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = read_data('/code/zhaoxudong03/RL/verl_base_zxd/01_llm_releate/data/tokenize.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(texts, trainer)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer_dir = '/code/zhaoxudong03/RL/verl_base_zxd/01_llm_releate/train_llm/tokenizer'\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, 'tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 vocab.json 和 merges.txt 文件\n",
    "tokenizer.model.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": True,\n",
    "        \"added_tokens_decoder\": {\n",
    "            \"0\": {\n",
    "                \"content\": \"<unk>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"1\": {\n",
    "                \"content\": \"<s>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"2\": {\n",
    "                \"content\": \"</s>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            }\n",
    "        },\n",
    "        \"additional_special_tokens\": [],\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"legacy\": True,\n",
    "        \"model_max_length\": 100000,\n",
    "        \"pad_token\": None,\n",
    "        \"sp_model_kwargs\": {},\n",
    "        \"spaces_between_special_tokens\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"use_default_system_prompt\": False,\n",
    "        \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "    }\n",
    "\n",
    "# 保存配置文件\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试\n",
    "print(tokenizer.encode(\"<pad>\"), tokenizer.encode(\"<unk>\"), tokenizer.encode(\"<s>\"), tokenizer.encode(\"</s>\"))\n",
    "print(tokenizer.decode(508))\n",
    "print(tokenizer.vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
